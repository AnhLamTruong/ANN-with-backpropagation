{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Convolution\n",
    "Take a small matrix of numbers(kernel) and pass it over the image to transform it based on filter values. After placing this kernel over the selected pixel, we take each value from the filter and multiply them in pairs with corresponding values from the image. Finally, we sum up and put the result in the right place in the output matrix. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def forward_pass(self, a_prev: np.array)->np.array:\n",
    "  #Pass it over the image to transform it based on filter values\n",
    "  n, height_in,width_in,_ = a_prev.shape\n",
    "  _, height_out,width_out,_ = output_shape\n",
    "  height_frames,width_frames,_,n_frames= self.width_shape\n",
    "  \n",
    "  #take each value from the filter and multiply them in pairs\n",
    "  output_shape=self.calculate_output_dims(input_dims=a_prev.shape)\n",
    "  output=np.zeros(output_shape)\n",
    "  \n",
    "  for i in range(height_out):\n",
    "    for j in range(width_out):\n",
    "      height_start,width_start= i , j\n",
    "      height_end, width_end= height_start + height_frames,width_start+width_frames\n",
    "      \n",
    "      #sum up and put the result in the right place in the output matrix\n",
    "      output[:, i , j,:] =np.sum(a_prev[:,height_start:height_end, width_start:width_end,:,np.newaxis]*self._w[np.newaxis,:,:,:],axis=(1,2,3))\n",
    "  \n",
    "  return output + self._b"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Max-Pooling\n",
    "The main task of the pooling layer is to reduce the spatial size of our tensor. This task is used to limit the number of parameters that need to be trained to shorten the whole training process.\n",
    "1. Deviding the tenintosor  sections.\n",
    "2. Applying the function to each part separeatly.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from turtle import width\n",
    "\n",
    "\n",
    "def forward_pass(self, array_prev:np.array)->np.array:\n",
    "  self._shape = array_prev.shape\n",
    "  n, height_in, width_in,c= array_prev.shape\n",
    "  height_pool, width_pool=self.pool_size\n",
    "  height_out=1+(height_in-height_pool)//self._stride\n",
    "  width_out=1+(width_in-width_pool)//self._stride\n",
    "  output = np.zeros((n, height_out,width_out,c))\n",
    "  \n",
    "  for i in range(height_out):\n",
    "    for j in range(width_out):\n",
    "      height_start=i*self._stride\n",
    "      height_end=height_start+height_pool\n",
    "      width_start=j*self._stride\n",
    "      width_end=width_start+width_pool\n",
    "      array_prev_slice=array_prev[:,height_start:height_end,width_start:width_end,:]\n",
    "      self._save_mask(x=array_prev_slice,cords=(i,j))\n",
    "      output[:,i,j,:]=np.max(array_prev_slice,axis=(1,2))\n",
    "  return output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### ReLU Layer\n",
    "Without activations, NN would become a combination of linear functions so that it would be just a linear function itself."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def forward_pass(self, array_prev: np.array)->np.array:\n",
    "  self._z =np.maximum(0,array_prev)\n",
    "  return self._z\n",
    "def backward_pass(self, array_prev: np.array)->np.array:\n",
    "  dz=np.array(array_prev,copy=True)\n",
    "  dz[self._z<=0]=0\n",
    "  return dz"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
